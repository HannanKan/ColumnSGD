package pku.mllibFP.classfication

import breeze.optimize.BatchSize
import org.apache.spark.SparkEnv
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import pku.mllibFP.util.{LabeledPartDataPoint, WorkSet}
import org.apache.spark.internal.Logging

import scala.reflect.ClassTag

/**
  * @param inputRDD      : dataRDD, each partition contains only one element, Array[IndexedDataPoint]
  * @param numFeatures
  * @param numPartitions : number of partitions for the model, e.g., number of tasks per stage
  * @param regParam
  * @param stepSize      : step size for batch
  * @param numIterations
  * @param miniBatchSize
  */
abstract class BaseFPModel[T: ClassTag](@transient inputRDD: RDD[WorkSet],
                                        numFeatures: Int,
                                        numPartitions: Int,
                                        regParam: Double,
                                        stepSize: Double,
                                        numIterations: Int,
                                        miniBatchSize: Int) extends Serializable with Logging {

  // if we send intermediate results, the benefit is that:
  // 1. the driver does nothing except computing the loss, the logic is clear.
  // 2. if we do low-precision, only one compression happens, i.e., compress the intermediate result.bcCoefficients.
  // BTW, for FM, we can sum G_f^2 on the driver and reduce the communication to B(K+1) from B(2*k+1)

  var intermediateResults: Array[Array[Double]] = null// Intermediate results, such as dot product, squared dot product.

  // initialize the intermediate result
  def iniInterResult(): Unit
  /**
    * [executed on executors]
    * generate the model, cache the data, compute the labels also new the intermediateResults Array
    * @param inputRDD
    * @return modelRDD, an combination of data and model.
    * */
  def generateModel(inputRDD: RDD[WorkSet]): RDD[(WorkSet, Array[Array[Double]])]

  /**
    * [executed on driver] compute the batch loss using the intermediate results gathered from executors.
    * @param interResults
    * @param labels
    * @param seed
    * @return
    */
  def computeBatchLoss(interResults: Array[Array[Double]], labels: Array[Double],
                       batchSize: Int = miniBatchSize, seed: Int): Double

  /**
    * [executed on executors] compute the intermediate results to be gathered to the driver[not gathered yet].
    * They could be dot product for linear models, (w, V) for factorization machine, (w1, ..., wk) for MLR.
    * @param model
    * @param work_set
    * @param new_seed
    * @return
    */
  def computeInterResults(model: Array[Array[Double]], work_set: WorkSet,
                          batchSize: Int = miniBatchSize, new_seed: Int): Array[Array[Double]]

  /**
    * [executed on executors]
    * update model using the intermediate results and sampled data from last_seed
    * @param model
    * @param work_set
    * @param interResults
    * @param last_seed
    */
  def updateModel(model: Array[Array[Double]], work_set: WorkSet, interResults: Array[Array[Double]],
                  batchSize: Int = miniBatchSize, last_seed: Int, iterationId: Int): Unit

  /**
    * update model using the intermediate results computed from samples generated by last seed.
    * compute the intermediate results from samples generated by the new seed.
    * combine update model and compute intermediate results (for example, dot products) as one action,
    * to reduce the scheduler delay.
    * @param modelRDD
    * @param bcInterResults
    * @param lastSeed
    * @param newSeed
    * @return "IntermediateResults"(not only dot product, but also other reduced results like S_f, G_f in factorization machines)
    */
  def updateModelAndComputeInterResults(modelRDD: RDD[(WorkSet, Array[Array[Double]])],
                                      bcInterResults: Broadcast[Array[Array[Double]]],
                                      lastSeed: Int, newSeed: Int, iterationId: Int): Array[Array[Double]] = {
    modelRDD.mapPartitions(
      iter => {
        val first_ele = iter.next()
        val workset: WorkSet = first_ele._1
        val model: Array[Array[Double]] = first_ele._2

        // update model first
        var worker_start_time = System.currentTimeMillis()
        updateL2Regu(model, regParam)
        updateModel(model, workset, bcInterResults.value, miniBatchSize, lastSeed, iterationId)
        logInfo(s"ghandFP=WorkerTime=updateModel:${(System.currentTimeMillis() - worker_start_time) / 1000.0}")

        // compute dot product
        worker_start_time = System.currentTimeMillis()
        val results: Array[Array[Double]] = computeInterResults(model, workset, miniBatchSize, newSeed)
        logInfo(s"ghandFP=WorkerTime=BatchDotProduct:${(System.currentTimeMillis() - worker_start_time) / 1000.0}")

        Iterator(results)

      }
    ).reduce(aggregateResult)
  }


  /**
    * regularization over model.
    * @param model
    * @param regParam
    */

  def updateL2Regu(model: Array[Array[Double]], regParam: Double): Unit = {
    if (regParam == 0)
      return

    val len1 = model.length
    val len2 = model(0).length
    var i, j =0
    while (i < len1) {
      j = 0
      while (j < len2){
        model(i)(j) *= (1 - regParam)
        j += 1
      }
      i += 1
    }
  }

  /**
    * aggregate intermediate results, which could be sum arrays in machine learning workloads
    * like LR, SVM, FM, MLR, etc.
    * @param array1
    * @param array2
    * @return
    */

  def aggregateResult(array1: Array[Array[Double]], array2: Array[Array[Double]]): Array[Array[Double]] = {
    assert(array1.length == array2.length)
    var k: Int = 0
    while (k < array1.length) {
      var i = 0
      while(i < array1(0).length){
        array1(k)(i) += array2(k)(i)
        i += 1
      }
      k += 1
    }
    array1
  }

  def valid(modelRDD: RDD[(WorkSet, Array[Array[Double]])],
            labels: Array[Double], validSize: Int): Double = {
    val interResult = modelRDD.mapPartitions(
      iter => {
        val tmp = iter.next
        val workset = tmp._1
        val model = tmp._2
        val local_result : Array[Array[Double]] = computeInterResults(model, workset, validSize, 100000)
        Iterator(local_result)
      }
    ).reduce(aggregateResult)

    val loss = computeBatchLoss(interResult, labels, validSize, 100000)
    loss
  }


  def miniBatchSGD(): Unit = {
    val start_loading = System.currentTimeMillis()
    val modelRDD: RDD[(WorkSet, Array[Array[Double]])] = generateModel(inputRDD)
    modelRDD.cache()
    modelRDD.setName("modelRDD")

    // collect labels to the driver
    val tmp: Array[(Int, Int, Array[Double])] = modelRDD.mapPartitionsWithIndex(
      (pid, iter) =>{
        val work_set: WorkSet = iter.next._1
        val num_data_points = work_set.getNumDataPoints()
        val worker_data_point_num = num_data_points / numPartitions + 1
        val start = worker_data_point_num * pid
        val end = math.min(worker_data_point_num + start, num_data_points)

        Iterator((pid, num_data_points, work_set.getLabels(start, end)))
      }
    ).collect()

    val labels: Array[Double] = new Array[Double](tmp(1)._2)
    val num_partitions = tmp.length
    val worker_data_num = labels.length / num_partitions + 1
    for(id <- 0 until(tmp.length)){
      val start = worker_data_num * tmp(id)._1
      val end = math.min(start + worker_data_num, labels.length)
      System.arraycopy(tmp(id)._3, 0, labels, start, end - start)
    }

    logInfo(s"ghand=loading:${(System.currentTimeMillis() - start_loading ) / 1000.0}")

    var start_time = System.currentTimeMillis()
    var iter_id: Int = 0
    var last_seed = 0
    var cur_seed = 0
    iniInterResult()
    while (iter_id < numIterations) {
      start_time = System.currentTimeMillis()
      // broadcast from coefficients last iteration
      cur_seed = 42 + iter_id
      last_seed = cur_seed - 1
      // compute loss from last iteration
      val batch_loss: Double = computeBatchLoss(intermediateResults, labels, miniBatchSize, last_seed)

      val valid_ratio = SparkEnv.get.conf.getDouble("spark.ml.validRatio", 0.01)
      val valid_loss: Double = valid(modelRDD, labels, (valid_ratio * labels.length).toInt)

      logInfo(s"ghandFP=DriverTime=evaluateBatchLossTime:" +
        s"${(System.currentTimeMillis() - start_time) / 1000.0}=BatchLoss:${batch_loss}, trainLoss:${valid_loss}")

      start_time = System.currentTimeMillis()
      val bcIntermediateResults = modelRDD.sparkContext.broadcast(intermediateResults)
      intermediateResults = updateModelAndComputeInterResults(modelRDD, bcIntermediateResults,
        last_seed, cur_seed, iter_id)
      iter_id += 1
      bcIntermediateResults.destroy()

      logInfo(s"ghandFP=DriverTime=trainTime:" +
        s"${(System.currentTimeMillis() - start_time) / 1000.0}")
    }

  }


  def miniBatchLBFGS(modelName: String): Unit = {

  }

}
