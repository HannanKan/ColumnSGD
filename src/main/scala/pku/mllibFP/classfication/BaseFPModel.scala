package pku.mllibFP.classfication

import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import pku.mllibFP.util.LabeledPartDataPoint
import org.apache.spark.internal.Logging

import scala.reflect.ClassTag

/**
  * @param inputRDD      : dataRDD, each partition contains only one element, Array[IndexedDataPoint]
  * @param numFeatures
  * @param numPartitions : number of partitions for the model, e.g., number of tasks per stage
  * @param regParam
  * @param stepSize      : step size for batch
  * @param numIterations
  * @param miniBatchSize
  */
abstract class BaseFPModel[T: ClassTag](@transient inputRDD: RDD[Array[LabeledPartDataPoint]],
                                        numFeatures: Int,
                                        numPartitions: Int,
                                        regParam: Double,
                                        stepSize: Double,
                                        numIterations: Int,
                                        miniBatchSize: Int) extends Serializable with Logging {

  // if we send intermediate results, the benefit is that:
  // 1. the driver does nothing except computing the loss, the logic is clear.
  // 2. if we do low-precision, only one compression happens, i.e., compress the intermediate result.bcCoefficients.
  // BTW, for FM, we can sum G_f^2 on the driver and reduce the communication to B(K+1) from B(2*k+1)

  var intermediateResults: Array[Array[Double]] = null// Intermediate results, such as dot product, squared dot product.
  /**
    * [executed on executors]
    * generate the model, cache the data, compute the labels also new the intermediateResults Array
    * @param inputRDD
    * @return modelRDD, an combination of data and model.
    * */
  def generateModel(inputRDD: RDD[Array[LabeledPartDataPoint]]): RDD[(Array[LabeledPartDataPoint], Array[Array[Double]])]

  /**
    * [executed on driver] compute the batch loss using the intermediate results gathered from executors.
    * @param interResults
    * @param labels
    * @param seed
    * @return
    */
  def computeBatchLoss(interResults: Array[Array[Double]], labels: Array[Double], seed: Int): Double

  /**
    * [executed on executors] compute the intermediate results to be gathered to the driver[not gathered yet].
    * They could be dot product for linear models, (w, V) for factorization machine, (w1, ..., wk) for MLR.
    * @param model
    * @param data_points
    * @param new_seed
    * @return
    */
  def computeInterResults(model: Array[Array[Double]], data_points: Array[LabeledPartDataPoint], new_seed: Int): Array[Array[Double]]

  /**
    * [executed on executors]
    * update model using the intermediate results and sampled data from last_seed
    * @param model
    * @param data_points
    * @param interResults
    * @param last_seed
    */
  def updateModel(model: Array[Array[Double]], data_points: Array[LabeledPartDataPoint],
                  interResults: Array[Array[Double]], last_seed: Int): Unit

  /**
    * update model using the intermediate results computed from samples generated by last seed.
    * compute the intermediate results from samples generated by the new seed.
    * combine update model and compute intermediate results (for example, dot products) as one action,
    * to reduce the scheduler delay.
    * @param modelRDD
    * @param bcInterResults
    * @param lastSeed
    * @param newSeed
    * @return "IntermediateResults"(not only dot product, but also other reduced results like S_f, G_f in factorization machines)
    */
  def updateModelAndComputeInterResults(modelRDD: RDD[(Array[LabeledPartDataPoint], Array[Array[Double]])],
                                      bcInterResults: Broadcast[Array[Array[Double]]],
                                      lastSeed: Int, newSeed: Int): Array[Array[Double]] = {
    modelRDD.mapPartitions(
      iter => {
        val first_ele = iter.next()
        val data_points: Array[LabeledPartDataPoint] = first_ele._1
        val model: Array[Array[Double]] = first_ele._2

        // update model first
        var worker_start_time = System.currentTimeMillis()
        updateL2Regu(model, regParam)
        updateModel(model, data_points, bcInterResults.value, lastSeed)
        logInfo(s"ghandFP=WorkerTime=updateModel:${(System.currentTimeMillis() - worker_start_time) / 1000.0}")

        // compute dot product
        worker_start_time = System.currentTimeMillis()
        val results: Array[Array[Double]] = computeInterResults(model, data_points, newSeed)
        logInfo(s"ghandFP=WorkerTime=BatchDotProduct:${(System.currentTimeMillis() - worker_start_time) / 1000.0}")

        Iterator(results)

      }
    ).reduce(aggregateResult)
  }


  /**
    * regularization over model.
    * @param model
    * @param regParam
    */

  def updateL2Regu(model: Array[Array[Double]], regParam: Double): Unit = {
    if (regParam == 0)
      return

    val len1 = model.length
    val len2 = model(0).length
    var i, j =0
    while (i < len1) {
      j = 0
      while (j < len2){
        model(i)(j) *= (1 - regParam)
        j += 1
      }
      i += 1
    }
  }

  /**
    * aggregate intermediate results, which could be sum arrays in machine learning workloads
    * like LR, SVM, FM, MLR, etc.
    * @param array1
    * @param array2
    * @return
    */

  def aggregateResult(array1: Array[Array[Double]], array2: Array[Array[Double]]): Array[Array[Double]] = {
    assert(array1.length == array2.length)
    var k: Int = 0
    while (k < array1.length) {
      var i = 0
      while(i < array1(0).length){
        array1(k)(i) += array2(k)(i)
        i += 1
      }
      k += 1
    }
    array1
  }


  def miniBatchSGD(): Unit = {
    val modelRDD: RDD[(Array[LabeledPartDataPoint], Array[Array[Double]])] = generateModel(inputRDD)
    modelRDD.cache()
    modelRDD.setName("modelRDD")

    // collect labels to the driver
    var labels: Array[Double] = null
    val tmp: Array[Array[Double]] = modelRDD.mapPartitionsWithIndex(
      (pid, iter) =>{
        if(pid == 0){
          val iter_array: Array[LabeledPartDataPoint] = iter.next._1
          val ll: Array[Double] = new Array[Double](iter_array.length)
          var i = 0
          while (i < ll.length){
            ll(i) = iter_array(i).label
            i += 1
          }
          Iterator(ll)
        }
        else
          Iterator(null)
      }
    ).collect()
    var i = 0
    while(i < tmp.length) {
      if (tmp(i) != null){
        labels = tmp(i)
      }
      i += 1
    }
    modelRDD.count()


    var start_time = System.currentTimeMillis()
    var iter_id: Int = 0
    var last_seed = 0
    var cur_seed = 0
    while (iter_id < numIterations) {
      start_time = System.currentTimeMillis()
      // broadcast from coefficients last iteration
      cur_seed = 42 + iter_id
      last_seed = cur_seed - 1
      // compute loss from last iteration
      val loss: Double = computeBatchLoss(intermediateResults, labels, last_seed)
      logInfo(s"ghandFP=DriverTime=evaluateBatchLoss:" +
        s"${(System.currentTimeMillis() - start_time) / 1000.0}=BatchLoss:${loss}")

      start_time = System.currentTimeMillis()
      val bcIntermediateResults = modelRDD.sparkContext.broadcast(intermediateResults)
      intermediateResults = updateModelAndComputeInterResults(modelRDD, bcIntermediateResults, last_seed, cur_seed)
      start_time = System.currentTimeMillis()
      iter_id += 1
      bcIntermediateResults.destroy()

      logInfo(s"ghandFP=DriverTime=trainTime:" +
        s"${(System.currentTimeMillis() - start_time) / 1000.0}")
    }

  }


  def miniBatchLBFGS(modelName: String): Unit = {

  }

}
