package pku.mllibFP.classfication

import org.apache.spark.{SparkEnv, TaskContext}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import pku.mllibFP.util._
import org.apache.spark.internal.Logging

import scala.reflect.ClassTag

/**
  * @param inputRDD      : dataRDD, each partition contains only one element, Array[IndexedDataPoint]
  * @param numFeatures
  * @param numPartitions : number of partitions for the model, e.g., number of tasks per stage
  * @param regParam
  * @param stepSize      : step size for batch
  * @param numIterations
  * @param miniBatchSize
  */
abstract class BaseFPModel[T: ClassTag](@transient inputRDD: RDD[ArrayWorkSet[WorkSet]],
                                        numFeatures: Int,
                                        numPartitions: Int,
                                        regParam: Double,
                                        stepSize: Double,
                                        numIterations: Int,
                                        miniBatchSize: Int) extends Serializable with Logging {

  // if we send intermediate results, the benefit is that:
  // 1. the driver does nothing except computing the loss, the logic is clear.
  // 2. if we do low-precision, only one compression happens, i.e., compress the intermediate result.bcCoefficients.
  // BTW, for FM, we can sum G_f^2 on the driver and reduce the communication to B(K+1) from B(2*k+1)

  var intermediateResults: Array[Array[Double]] = null// Intermediate results, such as dot product, squared dot product.

  // initialize the intermediate result
  def iniInterResult(): Unit
  /**
    * [executed on executors]
    * generate the model, cache the data, compute the labels also new the intermediateResults Array
    * @param inputRDD
    * @return modelRDD, an combination of data and model.
    * */
  def generateModel(inputRDD: RDD[ArrayWorkSet[WorkSet]]): RDD[(ArrayWorkSet[WorkSet], Array[Array[Double]])]

  /**
    * [executed on driver] compute the batch loss using the intermediate results gathered from executors.
    * @param interResults
    * @param labels
    * @param seed
    * @return
    */
  def computeBatchLoss(interResults: Array[Array[Double]], labels: ArrayLabels[Double],
                       batchSize: Int = miniBatchSize, seed: Int): Double

  /**
    * [executed on executors] compute the intermediate results to be gathered to the driver[not gathered yet].
    * They could be dot product for linear models, (w, V) for factorization machine, (w1, ..., wk) for MLR.
    * @param model
    * @param arrayWorkSet
    * @param new_seed
    * @return
    */
  def computeInterResults(model: Array[Array[Double]], arrayWorkSet: ArrayWorkSet[WorkSet],
                          batchSize: Int = miniBatchSize, new_seed: Int): Array[Array[Double]]

  /**
    * [executed on executors]
    * update model using the intermediate results and sampled data from last_seed
    * @param model
    * @param arrayWorkSet
    * @param interResults
    * @param last_seed
    */
  def updateModel(model: Array[Array[Double]], arrayWorkSet: ArrayWorkSet[WorkSet], interResults: Array[Array[Double]],
                  batchSize: Int = miniBatchSize, last_seed: Int, iterationId: Int): Unit

  /**
    * update model using the intermediate results computed from samples generated by last seed.
    * compute the intermediate results from samples generated by the new seed.
    * combine update model and compute intermediate results (for example, dot products) as one action,
    * to reduce the scheduler delay.
    * @param modelRDD
    * @param bcInterResults
    * @param lastSeed
    * @param newSeed
    * @return "IntermediateResults"(not only dot product, but also other reduced results like S_f, G_f in factorization machines)
    */
  def updateModelAndComputeInterResults(modelRDD: RDD[(ArrayWorkSet[WorkSet], Array[Array[Double]])],
                                      bcInterResults: Broadcast[Array[Array[Double]]],
                                      lastSeed: Int, newSeed: Int, iterationId: Int): Array[Array[Double]] = {
    modelRDD.mapPartitions(
      iter => {
        val first_ele = iter.next()
        val arrayWorkSet: ArrayWorkSet[WorkSet] = first_ele._1
        val model: Array[Array[Double]] = first_ele._2

        // manually throw an exception to model task failure
//        if(TaskContext.getPartitionId() == 0 && iterationId == 50 && ColumnMLConf.ExceptionCnt == 1){
//          ColumnMLConf.ExceptionCnt -= 1
//          logInfo(s"ghand=TaskException happens here")
//          throw new ColumnMLTaskException
//        }

        if(SparkEnv.get.conf.get("spark.ml.straggler", "false").toBoolean) {
          if(TaskContext.getPartitionId() == 0)
            Thread.sleep(300)
        }

        // update model first
        var worker_start_time = System.currentTimeMillis()
        updateL2Regu(model, regParam)
        updateModel(model, arrayWorkSet, bcInterResults.value, miniBatchSize, lastSeed, iterationId)
        logInfo(s"ghandFP=WorkerTime=updateModel:${(System.currentTimeMillis() - worker_start_time) / 1000.0}")

        // compute dot product
        worker_start_time = System.currentTimeMillis()
        val results: Array[Array[Double]] = computeInterResults(model, arrayWorkSet, miniBatchSize, newSeed)
        logInfo(s"ghandFP=WorkerTime=BatchDotProduct:${(System.currentTimeMillis() - worker_start_time) / 1000.0}")

        Iterator(results)

      }
    ).reduce(aggregateResult)
  }


  /**
    * regularization over model.
    * @param model
    * @param regParam
    */

  def updateL2Regu(model: Array[Array[Double]], regParam: Double): Unit = {
    if (regParam == 0)
      return

    val len1 = model.length
    val len2 = model(0).length
    var i, j =0
    while (i < len1) {
      j = 0
      while (j < len2){
        model(i)(j) *= (1 - regParam)
        j += 1
      }
      i += 1
    }
  }

  /**
    * aggregate intermediate results, which could be sum arrays in machine learning workloads
    * like LR, SVM, FM, MLR, etc.
    * @param array1
    * @param array2
    * @return
    */

  def aggregateResult(array1: Array[Array[Double]], array2: Array[Array[Double]]): Array[Array[Double]] = {
    assert(array1.length == array2.length)
    var k: Int = 0
    while (k < array1.length) {
      var i = 0
      while(i < array1(0).length){
        array1(k)(i) += array2(k)(i)
        i += 1
      }
      k += 1
    }
    array1
  }

  def valid(modelRDD: RDD[(ArrayWorkSet[WorkSet], Array[Array[Double]])],
            labels: ArrayLabels[Double], validSize: Int): Double = {
    val interResult = modelRDD.mapPartitions(
      iter => {
        val tmp = iter.next
        val arrayWorkSet = tmp._1
        val model = tmp._2
        val local_result : Array[Array[Double]] = computeInterResults(model, arrayWorkSet, validSize, 100000)
        Iterator(local_result)
      }
    ).reduce(aggregateResult)

    val loss = computeBatchLoss(interResult, labels, validSize, 100000)
    loss
  }


  def miniBatchSGD(): Unit = {
    val start_loading = System.currentTimeMillis()
    val modelRDD: RDD[(ArrayWorkSet[WorkSet], Array[Array[Double]])] = generateModel(inputRDD)
    modelRDD.cache()
    modelRDD.setName("modelRDD")

    // do checkpoint for fault tolerance.
//    if(ColumnMLConf.ExceptionCnt == 1) {
//      val ckdir = SparkEnv.get.conf.get("spark.checkpointDir", "checkpoint")
//      modelRDD.sparkContext.setCheckpointDir(ckdir)
//      modelRDD.checkpoint()
//    }

    // collect labels to the driver, orangized as grouped labels
    val tmp: Array[(Int, Array[Double])] = modelRDD.mapPartitionsWithIndex(
      (pid, iter) =>{
        val arrayWorkSet: ArrayWorkSet[WorkSet] = iter.next._1
        val numWorkSets = arrayWorkSet.length()
        val worker_partition_num = numWorkSets / numPartitions + 1
        val start = worker_partition_num * pid
        val end = math.min(worker_partition_num + start, numWorkSets)
        if(end > start) {
          val partitionIds: Array[Int] = new Array[Int](end - start)
          for (id <- 0 until (partitionIds.length)) {
            partitionIds(id) = id + start
          }
          arrayWorkSet.getLabels(partitionIds).toIterator
        }
        else
          Array.empty[(Int, Array[Double])].toIterator
      }
    ).collect()

    val labels_tmp: Array[Array[Double]] = new Array[Array[Double]](tmp.length)
    for(id <- 0 until(labels_tmp.length)){
      labels_tmp(tmp(id)._1) = tmp(id)._2
    }
    val labels = new ArrayLabels[Double](labels_tmp)

    logInfo(s"ghand=loading:${(System.currentTimeMillis() - start_loading ) / 1000.0}")

    var start_time = System.currentTimeMillis()
    var iter_id: Int = 0
    var last_seed = 0
    var cur_seed = 0
    iniInterResult()
    while (iter_id < numIterations) {
      start_time = System.currentTimeMillis()
      // broadcast from coefficients last iteration
      cur_seed = 42 + iter_id
      last_seed = cur_seed - 1
      // compute loss from last iteration
      val batch_loss: Double = computeBatchLoss(intermediateResults, labels, miniBatchSize, last_seed)
      val valid_ratio = SparkEnv.get.conf.getDouble("spark.ml.validRatio", 0.01)
      var valid_loss: Double = 0

      // set the valid ratio for debugging.
      if (iter_id % 1000 == 0) {
        valid_loss = valid(modelRDD, labels, (valid_ratio * labels.numLabels).toInt)
      }

      logInfo(s"ghandFP=DriverTime=evaluateBatchLossTime:" +
        s"${(System.currentTimeMillis() - start_time) / 1000.0}=BatchLoss:${batch_loss}=trainLoss:${valid_loss}")

      start_time = System.currentTimeMillis()
      val bcIntermediateResults = modelRDD.sparkContext.broadcast(intermediateResults)
      intermediateResults = updateModelAndComputeInterResults(modelRDD, bcIntermediateResults,
        last_seed, cur_seed, iter_id)
      iter_id += 1
      bcIntermediateResults.destroy()

      logInfo(s"ghandFP=DriverTime=trainTime:" +
        s"${(System.currentTimeMillis() - start_time) / 1000.0}")
    }

  }


  def miniBatchLBFGS(modelName: String): Unit = {

  }

}
